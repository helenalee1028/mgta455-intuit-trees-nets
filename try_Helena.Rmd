### Decision Tree - loosen the control

#### Data import
```{r}
## loading the data. Note that data must be loaded from the data/
## in the rstudio project directory
intuit75k <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/intuit75k.rds"))
```

```{r include=FALSE}
library(tidyverse)
library(rpart)
library(ipred)
library(gbm)
library(caret)
```

```{r}
## change variable types for 5 factor variables
intuit75k <- intuit75k %>% 
  mutate(zip801 = ifelse(zip == "00801", "Yes", "No"),
         zip804 = ifelse(zip == "00804", "Yes", "No"))

intuit75k <- mutate_at(intuit75k, .vars = vars(zip_bins, bizflag, version1, owntaxprod, upgraded, zip801, zip804), .funs = funs(as.factor))

train = intuit75k %>%
  filter(training == 1)

test = intuit75k %>%
  filter(training == 0)
```

```{r}
decision_tree = rpart(formula = res1 ~ zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train, control=rpart.control(minsplit=2, minbucket=1, cp=0.001))

test$decision_tree_pred = predict(decision_tree, newdata = test, type = 'prob')

```

### Bagging
```{r}
bagging = bagging(formula = res1 ~  zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train, nbagg=50)
test$bagging_pred = predict(object = bagging, newdata = test, type = 'prob')
```

### Treebag cross-validation
```{r}
ctrl = trainControl(method = 'cv', number = 5)
treebag = train(res1 ~ zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train, method = 'treebag', trControl = ctrl)
test$treebag_pred = predict(object = treebag, newdata = test, type = 'prob')$`Yes`
```

### GBM - default setting
```{r}
test_gbm = test
train_gbm = train
train_gbm$res1 <- ifelse(train_gbm$res1 == "Yes", 1, 0)
test_gbm$res1 <- ifelse(test_gbm$res1 == "Yes", 1, 0)

gbm = gbm(formula = res1 ~  zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, distribution = 'bernoulli', data = train_gbm, n.trees = 100)
test$gbm_pred = predict.gbm(object = gbm, newdata = test_gbm, type = 'response',n.trees = 100)
```


```{r}
vars_vec = c('decision_tree_pred','bagging_pred','treebag_pred','gbm_pred')

basic_model_eval = model_eval(test,vars_vec)
saveRDS(basic_model_eval, "data/basic_model_eval.rds")
cm(test,vars_vec)
lift_gain_plot(test,vars_vec)
basic_model_eval <- readRDS("data/basic_model_eval.rds")
basic_model_eval
```

The model with outstanding profits is gbm, so we will focus on tuning the hyparameters of this model.

```{r eval = FALSE}
# hyper parameter tuning
interaction.depth <- c(2,4,6)
shrinkage <- c(0.001, 0.01, 0.1)
cv.folds <- c(2,4,6)

# build parameters matrix and keep auc performance
params <- expand.grid(interaction.depth, shrinkage, cv.folds)
params$auc <- NA
colnames(params) <- c("interaction.depth", "shrinkage", "cv.folds", "auc")

train$res1 <- ifelse(train$res1 == "Yes", 1, 0)
test$res1 <- ifelse(test$res1 == "Yes", 1, 0)

train = train[,-c(1,2)]
test = test[,-c(1,2)]

# train gbm models and keep track of test data performance
for (i in 1:nrow(params)){
  set.seed(123)
  gbm_tune <- gbm(formula = res1 ~ . , 
                  data = train, 
                  distribution = "bernoulli", 
                  interaction.depth = params[i,1],
                  shrinkage = params[i,2],
                  cv.folds = params[i,3],
                  n.trees = 100)
  print(i)
  pred_tune <- predict(gbm_tune, newdata = test, type = "response", n.trees = 100)
  auc <- auc(pred_tune, test$res1)
  params[i,4] <- auc

}

saveRDS(params, "gbm_tune.rds")

```

```{r}
gbm_tune = readRDS("data/gbm_tune.rds")
gbm_tune %>% 
  arrange(desc(auc)) %>% 
  top_n(auc, 10)

```

Based on our previous tuning results, we'll split our parameters into smaller units and train our models.

```{r eval = FALSE}

# hyper parameter tuning
interaction.depth <- c(3,4,5)
shrinkage <- c(0.07,0.08,0.09,0.1)
cv.folds <- c(6)

# build parameters matrix and keep auc performance
params2 <- expand.grid(interaction.depth, shrinkage, cv.folds)
params2$auc <- NA
colnames(params2) <- c("interaction.depth", "shrinkage", "cv.folds", "auc")

# train gbm models and keep track of test data performance
for (i in 1:nrow(params2)){
  set.seed(123)
  gbm_tune2 <- gbm(formula = res1 ~ . , 
                   data = train, 
                   distribution = "bernoulli", 
                   interaction.depth = params2[i,1],
                   shrinkage = params2[i,2],
                   cv.folds = params2[i,3],
                   n.trees = 100)
  print(i)
  pred_tune2 <- predict(gbm_tune2, newdata = test, type = "response", n.trees = 100)
  auc2 <- ModelMetrics::auc(test$res1,pred_tune2)
  params2[i,4] <- auc2
  
}

saveRDS(params2, "gbm_tune2.rds")
```

```{r}
params2 <- readRDS("data/gbm_tune2.rds")
params2 %>% 
  arrange(desc(auc))

```

After 2nd round of hyperparameters tuning, we decide to use 8 top performing gbm models in terms of auc and calculate expected profits, since higher AUC doesn't necessarily mean higher profit.

```{r eval = FALSE}
optimal_params_gbm <- params2 %>% 
arrange(desc(auc)) %>% 
top_n(auc, n = 8) %>% 
select(-auc)

train_gbm = train_gbm[,-c(1,2)]
test_gbm = test_gbm[,-c(1,2)]

gbm_preds <- matrix(NA, ncol = nrow(optimal_params_gbm), nrow = nrow(test_gbm))

for (i in 1:nrow(optimal_params_gbm)){
set.seed(123)
gbm_tune <- gbm(formula = res1 ~ . , 
data = train_gbm, 
distribution = "bernoulli", 
interaction.depth = optimal_params_gbm[i,1],
shrinkage = optimal_params_gbm[i,2],
n.trees = 100)
print(i)
pred_tune <- predict(gbm_tune, newdata = test_gbm, type = "response", n.trees = 100)
gbm_preds[,i] <- pred_tune
}


# cbind the predictions to intuit75_test dataframe and put into evaluations
colnames(gbm_preds) <- paste(rep("gbm", 8), c(1:8))
test_gbm1 <- cbind(test_gbm, gbm_preds)

vars <- tail(colnames(test_gbm1), 8)
test_gbm1$res1 <- ifelse(test_gbm1$res1 == 1, 'Yes', 'No')

model_eval(test_gbm1, vars)
cm(test_gbm1, vars)
lift_gain_plot(test_gbm1, vars)
gbm_tune1_model_eval = model_eval(test_gbm1, vars)
saveRDS(gbm_tune1_model_eval, "data/gbm_tune1_model_eval.rds")

```

```{r}
gbm_tune1_model_eval <- readRDS("data/gbm_tune1_model_eval.rds")
gbm_tune1_model_eval

```

Since in previous hyperparameter tuning process, we only tried n.trees of 100 and 1000, and the latter's performance is much lower than the former. Next we also tried to set n.trees to 200 to train our model. 

```{r eval= FALSE}
optimal_params_gbm2 <- params2 %>% 
arrange(desc(auc)) %>% 
top_n(auc, n = 8) %>% 
select(-auc)

gbm_preds2 <- matrix(NA, ncol = nrow(optimal_params_gbm2), nrow = nrow(test_gbm))

for (i in 1:nrow(optimal_params_gbm2)){
set.seed(123)
gbm_tune <- gbm(formula = res1 ~ . , 
data = train_gbm, 
distribution = "bernoulli", 
interaction.depth = optimal_params_gbm2[i,1],
shrinkage = optimal_params_gbm2[i,2],
n.trees = 200)
print(i)
pred_tune <- predict(gbm_tune, newdata = test_gbm, type = "response", n.trees = 200)
gbm_preds2[,i] <- pred_tune
}

# cbind the predictions to intuit75_test dataframe and put into evaluations
colnames(gbm_preds2) <- paste(rep("gbm2", 8), c(1:8))
test_gbm2 <- cbind(test_gbm, gbm_preds2)

vars2 <- tail(colnames(test_gbm2), 8)
test_gbm2$res1 <- ifelse(test_gbm2$res1 == 1, 'Yes', 'No')
model_eval(test_gbm2, vars2)
cm(test_gbm2, vars2)
lift_gain_plot(test_gbm2, vars2)
gbm_tune2_model_eval = model_eval(test_gbm2, vars2)
saveRDS(gbm_tune2_model_eval, "data/gbm_tune2_model_eval.rds")

```

```{r}
gbm_tune2_model_eval <- readRDS("data/gbm_tune2_model_eval.rds")
gbm_tune2_model_eval
```

Thus we can see that using 100 n.trees is better than 200 trees, so for the final model we choose to use 100 n.trees to train the data to prevent overfitting. So the best model we select has the highest profit of $460,358.59 on the test set. It has the hyperparameters of 3 interaction depth, shrinkage of 0.1.
