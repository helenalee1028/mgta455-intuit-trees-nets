---
title: "Helena"
author: "Wenrui_Li"
date: "2/18/2019"
output: html_document
---

```{r r_setup, include = FALSE}
## initial settings
knitr::opts_chunk$set(
  comment = NA,
  echo = TRUE,
  error = TRUE,
  cache = FALSE,
  message = FALSE,
  dpi = 144,
  warning = FALSE
)

## width to use when printing tables etc.
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE
)

## load radiant packages if needed
if (!exists("r_environment")) library(radiant)
```

<style>
.table {
  width: auto;
}
ul, ol {
  padding-left: 18px;
}
pre, code, pre code {
  overflow: auto;
  white-space: pre;
  word-wrap: normal;
  background-color: #ffffff;
}
</style>
```{r}
## loading the data. Note that data must be loaded from the data/
## in the rstudio project directory
intuit75k <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/intuit75k.rds"))
```

```{r}
library(tidyverse)
library(rpart)
library(ipred)
library(caret)
library(randomForest)
```


```{r}
## change variable types for 5 factor variables

intuit75k <- intuit75k %>% 
  mutate(zip801 = ifelse(zip == "00801", "Yes", "No"),
         zip804 = ifelse(zip == "00804", "Yes", "No"))

intuit75k <- mutate_at(intuit75k, .vars = vars(zip_bins, bizflag, version1, owntaxprod, upgraded, zip801, zip804), .funs = funs(as.factor))

class(intuit75k$zip801)

train = intuit75k %>%
  filter(training == 1)
  
test = intuit75k %>%
  filter(training == 0)
```

Decision Tree (loosen the control)
```{r}
decision_tree = rpart(formula = res1 ~ zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train, control=rpart.control(minsplit=2, minbucket=1, cp=0.001))
```

```{r}
test$decision_tree_pred = predict(decision_tree, newdata = test, type = 'prob')
```

## Bagging
```{r}
bagging = bagging(formula = res1 ~  zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train, nbagg=50)
test$bagging_pred = predict(object = bagging, newdata = test, type = 'prob')
```

## treebag cross-validation
```{r}
ctrl = trainControl(method = 'cv', number = 5)
treebag = train(res1 ~ zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train, method = 'treebag', trControl = ctrl)
```

## Random Forest
```{r}
random_forest = randomForest(formula = res1 ~  zip_bins + numords + dollars + last + version1 + owntaxprod + upgraded + zip801 + zip804, data = train)
test$random_forest_pred = predict(object = random_forest, newdata = test, type = 'prob')
```

## tuning random forest
```{r}
columns = c("zip_bins","numords","dollars","last","version1","owntaxprod","upgraded","zip801","zip804")
tuning_random_forest <- tuneRF(x = subset(train, select = columns),
              y = train$res1,
              ntreeTry = 500, doBest = TRUE)

test$tuning_random_forest_pred = predict(object = tuning_random_forest, newdata = test, type = 'prob')
```


```{r}
model_eval <- function(dat, vars){
  
  # calculate expected scaled profits and ROME
  
  eval_df <- as.data.frame(matrix(NA, ncol = 6, nrow = length(vars)))
  colnames(eval_df) <- c("var","nr_mail", "rep_rate_w2", "nr_resp_w2", "profit", "ROME")
  
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    
    mailto <- ifelse(pull(dat, !!var) > 2 * breakeven_rate, "TRUE", "FALSE") # decide whether to mail or not
    mailto_rate <- mean(mailto == "TRUE")
    nr_mail <- mailto_rate * (801821 - 38487) #target size
    
    resp <- pull(dat, "res1") # extract respondents
    
    rep_rate_w1 <- sum(resp == "Yes" & mailto == "TRUE")/sum(mailto=="TRUE") # response rate among targeted audience
    rep_rate_w2 <- rep_rate_w1 * 0.5
    nr_resp_w2 <- rep_rate_w2 * nr_mail # response customers among targeted audience
    
    sum_cost <- mail_cost * nr_mail
    revenue <- margin_revenue * nr_resp_w2
    profit <- revenue - sum_cost
    ROME <- profit/sum_cost
    
    perf_vec <- c(var, nr_mail, rep_rate_w2, nr_resp_w2, profit, ROME)
    
    eval_df[i,] <- perf_vec
  }
  
  return(eval_df) 
}

```

```{r}
mail_cost <- 1.41
margin_revenue <- 60
breakeven_rate <- mail_cost/margin_revenue
vars_vec = c('decision_tree_pred','bagging_pred','random_forest_pred','tuning_random_forest_pred')
model_eval(test,vars_vec)
```


```{r}
y_train = train$res1
X_train = train[,-13]
y_test = test$res1
X_test = test[,-13]
```


```{python}
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
```


```{python}
models = [GaussianNB(),
          SVC(random_state=5),
          RandomForestClassifier(random_state=5, n_estimators=10),
          MLPClassifier(random_state=5)]
```


```{python}
grid_data = [{},
              {'kernel': ['rbf', 'sigmoid'], 'C': [0.1, 1, 10, 100], 'random_state': [5]},
              {'n_estimators': [10, 50, 100],
               'criterion': ['gini', 'entropy'],
               'max_depth': [None, 10, 50, 100],
               'min_samples_split': [2, 5, 10],
               'random_state': [5]},
              {'hidden_layer_sizes': [10, 50, 100],
               'activation': ['identity', 'logistic', 'tanh', 'relu'],
               'solver': ['lbfgs', 'sgd', 'adam'],
               'learning_rate': ['constant', 'invscaling', 'adaptive'],
               'max_iter': [200, 400, 800],
               'random_state': [5]}]
```


```{python}
print(models)
```
```{python}
models_grid = list()
for i in range(0,len(models)):
    print(i)
    grid = GridSearchCV(estimator = models[i], param_grid = grid_data[i], scoring='f1').fit(X_train, y_train) 
    # there is some problem here. cannot finish
    print(grid.best_params_)
    model = grid.best_estimator_
    models_grid.append(model)
```
```{r}
X_train = as.matrix(X_train)
X_train
```

```{python}

y_train = train['res1']
X_train = train[,-13]
y_test = test$res1
X_test = test[,-13]
```
```{python}
print(train)
```

