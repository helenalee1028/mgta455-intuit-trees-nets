---
title: "Jasmine"
author: "s1shi"
date: "2/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0219 Xgboost
```{r}
library(xgboost)
library(caret)
## create dataframe
temp <- dummyVars(~zip_bins +sex ,data = intuit75k_train)
pred <- predict(temp, newdata = intuit75k_train)
intuit75k_xgb_train <- intuit75k_train %>%
  cbind(pred) %>%
  select(-c('sex','zip_bins','zip_bins.1','sex.Female')) %>%
  mutate(zip801 = ifelse(zip801 == 'Yes',1,0),
         zip804 = ifelse(zip804 == 'Yes',1,0)) %>%
  mutate(upgraded = as.numeric(upgraded)-1,
         owntaxprod =as.numeric(upgraded)-1,
         version1 = as.numeric(version1)-1,
         bizflag =as.numeric(bizflag)-1,
         res1 = as.numeric(res1)-1)
## change bizflag, version1,owntaxprod,upgraded,res1 to numeric 

temp <- dummyVars(~zip_bins +sex ,data = intuit75k_test)
pred <- predict(temp, newdata = intuit75k_test)
intuit75k_xgb_test <- intuit75k_test %>%
  cbind(pred) %>%
  select(-c('sex','zip_bins','zip_bins.1','sex.Female')) %>%
  mutate(zip801 = ifelse(zip801 == 'Yes',1,0),
         zip804 = ifelse(zip804 == 'Yes',1,0)) %>%
  mutate(upgraded = as.numeric(upgraded)-1,
         owntaxprod =as.numeric(upgraded)-1,
         version1 = as.numeric(version1)-1,
         bizflag =as.numeric(bizflag)-1,
         res1 = as.numeric(res1)-1)
```
Train xgboost model

```{r}
X_train = model.matrix(res1~.,intuit75k_xgb_train)
y_train = intuit75k_xgb_train$res1

X_test = model.matrix(res1~.,intuit75k_xgb_test)
y_test = intuit75k_xgb_test$res1

dtest <- xgb.DMatrix(data = X_test, label = y_test)
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
```
## First tune best nrounds

```{r}
## First tune best nrounds
nrounds = c(20,30)
objective = "binary:logistic"
eta <- c(0.2,0.3)
max_depth <- c(6,7,8)
#subsample <- c(0.5)
#colsample_bytree <- c(0.5)
#gamma <- c(0,0.1,0.2)
#min_child_weight <- c(5)
#nrounds = c(20,30,50,100,200,500)
#eta <- c(0.1,0.2,0.3)
#max_depth <- c(4,5,6,7,8)
params <- expand.grid(nrounds,objective,eta,max_depth)
#params$auc <- NA
colnames(params) <- c("nrounds",'objective', "eta",'max_depth')


auc <- c()
train_auc <-c()
for (i in 1:nrow(params)){
 
  xgb <- xgb.train(
    nrounds = params[i,1],
    params = as.list(params[i,-1]),
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")
  auc_ <- tail(xgb$evaluation_log$val_auc,1)
  train_auc_ <- tail(xgb$evaluation_log$train_auc,1)
  auc<- c(auc,auc_)
  train_auc <-c(train_auc,train_auc_)
}
params$auc <- auc
params$train_auc <- train_auc
saveRDS(params, "xgb_tune.rds")

```
```{r}
params %>% 
  top_n(auc, n = 3) 
```

We then determine the best eta is 0.2, and max_depth is 6. Try other parameters.


```{r}
## First tune best nrounds
nrounds = c(20,30)
objective = "binary:logistic"
eta <- c(0.2)
max_depth <- c(6)
subsample <- seq(0.8,1,0.2)
colsample_bytree <- c(0.8)
gamma <- c(0)
min_child_weight <- seq(1,5,2)
#nrounds = c(20,30,50,100,200,500)
#eta <- c(0.1,0.2,0.3)
#max_depth <- c(4,5,6,7,8)
params <- expand.grid(nrounds,objective,eta,max_depth,subsample,colsample_bytree,gamma,min_child_weight)
#params$auc <- NA
colnames(params) <- c("nrounds",'objective', "eta",'max_depth','subsample','colsample_bytree','gamma','min_child_weight')


auc <- c()
train_auc <-c()
for (i in 1:nrow(params)){
 
  xgb <- xgb.train(
    nrounds = params[i,1],
    params = as.list(params[i,-1]),
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")
  auc_ <- tail(xgb$evaluation_log$val_auc,1)
  train_auc_ <- tail(xgb$evaluation_log$train_auc,1)
  auc<- c(auc,auc_)
  train_auc <-c(train_auc,train_auc_)
}
params$auc <- auc
params$train_auc <- train_auc
saveRDS(params, "xgb_tune_all.rds")
#xgb_tune <- xgb_tune[,-4]
```
```{r}
params %>% 
  top_n(auc, n = 10) 
```

From the result, the appropriate nrounds are 20 and 30,eta = 0.2,max_depth = 6 subsample is 1, colsamply_bytree is 0.8, gamma is 0. min_child_weight is 1.
```{r}
params <- list(
  objective = "binary:logistic",
  eta = 0.2,
  max_depth = 6,
  colsamply_bytree = 0.8,
  min_child_weight = 1
)


xgb <- xgb.train(
    nrounds = 20,
    params = params,
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 1,
    eval_metric = "auc")
AUC <- tail(xgb$evaluation_log$val_auc,1)

pred = predict(xgb, dtest)
intuit75k_test$`pred_xgb` <- pred
model_eval(intuit75k_test, 'pred_xgb')
```


```{r}



```
