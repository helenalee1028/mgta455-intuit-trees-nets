---
title: "Jasmine"
author: "s1shi"
date: "2/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0219 Xgboost
```{r}
library(xgboost)
library(caret)
## create dataframe
temp <- dummyVars(~zip_bins +sex ,data = intuit75k_train)
pred <- predict(temp, newdata = intuit75k_train)
intuit75k_xgb_train <- intuit75k_train %>%
  cbind(pred) %>%
  select(-c('sex','zip_bins','zip_bins.1','sex.Female')) %>%
  mutate(zip801 = ifelse(zip801 == 'Yes',1,0),
         zip804 = ifelse(zip804 == 'Yes',1,0)) %>%
  mutate(upgraded = as.numeric(upgraded),
         owntaxprod =as.numeric(upgraded),
         version1 = as.numeric(version1),
         bizflag =as.numeric(bizflag),
         res1 = as.numeric(res1)-1)
## change bizflag, version1,owntaxprod,upgraded,res1 to numeric 

temp <- dummyVars(~zip_bins +sex ,data = intuit75k_test)
pred <- predict(temp, newdata = intuit75k_test)
intuit75k_xgb_test <- intuit75k_test %>%
  cbind(pred) %>%
  select(-c('sex','zip_bins','zip_bins.1','sex.Female')) %>%
  mutate(zip801 = ifelse(zip801 == 'Yes',1,0),
         zip804 = ifelse(zip804 == 'Yes',1,0)) %>%
  mutate(upgraded = as.numeric(upgraded),
         owntaxprod =as.numeric(upgraded),
         version1 = as.numeric(version1),
         bizflag =as.numeric(bizflag),
         res1 = as.numeric(res1)-1)
```
Train xgboost model

```{r}
X_train = model.matrix(res1~.,intuit75k_xgb_train)
y_train = intuit75k_xgb_train$res1

X_test = model.matrix(res1~.,intuit75k_xgb_test)
y_test = intuit75k_xgb_test$res1

dtest <- xgb.DMatrix(data = X_test, label = y_test)
dtrain <- xgb.DMatrix(data = X_train, label = y_train)

params2 <- list(
  objective = "binary:logistic",
  eta = 0.3,
  max_depth = 4
)

xgbcv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 6,
  print_every_n = 10,
  eval_metric = "auc"
)

## the best n round is 16.
```
```{r}
#head(xgbcv$evaluation_log)
## Get best n rounds
#n_rounds <- arrange(xgbcv$evaluation_log, desc(test_auc_mean))[1, 'iter']
```


```{r}
xgb <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 20,
  watchlist = list(val = dtest, train = dtrain),
  print_every_n = 1,
  eval_metric = "auc"
)
head(xgbcv$evaluation_log)
arrange(xgbcv$evaluation_log, desc(test_auc_mean))[1,'test_auc_mean' ]
a <-tail(xgb$evaluation_log$val_auc,1)


```


```{r}
# hyper parameter tuning
nrounds = c(20,30)
eta <- c(0.2,0.3)
max_depth <- c(6,7,8)


#nrounds = c(20,30,50,100,200,500)
#eta <- c(0.1,0.2,0.3)
#max_depth <- c(4,5,6,7,8)
params <- expand.grid(nrounds,eta,max_depth)
#params$auc <- NA
#colnames(params) <- c("nrounds", "eta", "max_depth", "auc")


auc <- c()
for (i in 1:nrow(params)){
 
  xgb <- xgb.train(
    nrounds = params[i,1],
    params = as.list(params[i,-1]),
    data = dtrain,
    watchlist = list(val = dtest, train = dtrain),
    print_every_n = 10,
    eval_metric = "auc")
  auc_ <- tail(xgb$evaluation_log$val_auc,1)
  auc<- c(auc,auc_)
}
params$auc <- auc

```
